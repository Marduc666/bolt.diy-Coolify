# ======================================
# Coolify Environment Variables Template
# ======================================
# This is a simplified template for Coolify deployments
# Copy these variables to your Coolify environment configuration
# See COOLIFY_DEPLOYMENT.md for detailed setup instructions

# ======================================
# REQUIRED: AI PROVIDER (Choose at least one)
# ======================================

# Anthropic Claude (Recommended for best results)
# Get your API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=

# OpenAI GPT models
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# Groq (Fast inference, generous free tier)
# Get your API key from: https://console.groq.com/keys
GROQ_API_KEY=

# ======================================
# OPTIONAL: Additional AI Providers
# ======================================

# Google Gemini
GOOGLE_GENERATIVE_AI_API_KEY=

# DeepSeek
DEEPSEEK_API_KEY=

# Mistral
MISTRAL_API_KEY=

# Cohere
COHERE_API_KEY=

# X.AI (Grok)
XAI_API_KEY=

# Together AI
TOGETHER_API_KEY=

# Perplexity
PERPLEXITY_API_KEY=

# HuggingFace
HuggingFace_API_KEY=

# OpenRouter (Access multiple providers)
OPEN_ROUTER_API_KEY=

# Moonshot (Kimi)
MOONSHOT_API_KEY=

# Hyperbolic
HYPERBOLIC_API_KEY=

# GitHub Models
GITHUB_API_KEY=

# AWS Bedrock (JSON format)
# Example: {"region":"us-east-1","accessKeyId":"xxx","secretAccessKey":"xxx"}
AWS_BEDROCK_CONFIG=

# ======================================
# OPTIONAL: Local AI Providers
# ======================================

# Ollama (must be accessible from Coolify)
# Use host.docker.internal for same-machine deployments
OLLAMA_API_BASE_URL=

# LM Studio
LMSTUDIO_API_BASE_URL=

# OpenAI-compatible endpoints
OPENAI_LIKE_API_BASE_URL=
OPENAI_LIKE_API_KEY=

# Together AI Base URL (if custom)
TOGETHER_API_BASE_URL=

# Hyperbolic Base URL (default provided)
HYPERBOLIC_API_BASE_URL=https://api.hyperbolic.xyz/v1/chat/completions

# ======================================
# OPTIONAL: Integration Services
# ======================================

# GitHub Integration (for repo import/export)
VITE_GITHUB_ACCESS_TOKEN=
VITE_GITHUB_TOKEN_TYPE=classic

# GitLab Integration
VITE_GITLAB_ACCESS_TOKEN=
VITE_GITLAB_URL=https://gitlab.com
VITE_GITLAB_TOKEN_TYPE=personal-access-token

# Vercel Deployment
VITE_VERCEL_ACCESS_TOKEN=

# Netlify Deployment
VITE_NETLIFY_ACCESS_TOKEN=

# Supabase Integration
VITE_SUPABASE_URL=
VITE_SUPABASE_ANON_KEY=
VITE_SUPABASE_ACCESS_TOKEN=

# ======================================
# APPLICATION SETTINGS
# ======================================

# Environment (production recommended for Coolify)
NODE_ENV=production

# Logging level (info recommended for production)
# Options: debug, info, warn, error
VITE_LOG_LEVEL=info

# Context window for local models
DEFAULT_NUM_CTX=32768

# Port (Coolify auto-detects, but can be set explicitly)
PORT=5173

# Public URL (Coolify sets this automatically)
# Leave empty unless you need to override
VITE_PUBLIC_APP_URL=

# ======================================
# COOLIFY DEPLOYMENT NOTES
# ======================================
# 
# 1. In Coolify, navigate to your application
# 2. Go to "Environment Variables" tab
# 3. Add the variables you need (at minimum one AI provider)
# 4. Click "Save" and redeploy
#
# For bulk import:
# - Click "Bulk Edit" in Coolify
# - Paste your KEY=VALUE pairs
# - Click "Save"
#
# See COOLIFY_DEPLOYMENT.md for complete setup guide
